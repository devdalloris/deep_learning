{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X = housing['data']\n",
    "y = housing['target']\n",
    "\n",
    "std = StandardScaler()\n",
    "X_scaled = std.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0]], shape=(20640, 10))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FNN: # Feedforward Neural Network\n",
    "    def __init__(self, input_size: int, hidden_sizes: list[int], output_size: int):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        layer_sizes = [self.input_size] + self.hidden_sizes + [self.output_size]\n",
    "        self.Ws = []\n",
    "        self.bs = []\n",
    "\n",
    "        for size1, size2 in zip(layer_sizes, layer_sizes[1:]):\n",
    "            W = np.random.randn(size1, size2)\n",
    "            b = np.random.randn(size2)\n",
    "            self.Ws.append(W)\n",
    "            self.bs.append(b)\n",
    "\n",
    "        # and we have to fit them ...\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        h_in = X\n",
    "        for W, b in zip(self.Ws, self.bs):\n",
    "            z = h_in @ W + b\n",
    "            h_out = heaviside(z)\n",
    "            h_in = h_out\n",
    "\n",
    "        return h_out\n",
    "\n",
    "\n",
    "nn = FNN(input_size=8, hidden_sizes=[6, 4, 2, 7], output_size=10)\n",
    "nn.fit(X, y)\n",
    "nn.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class FNN:\n",
    "    def __init__(self, input_size: int, hidden_sizes: list[int], output_size: int):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.Ws = []\n",
    "        self.bs = []\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, learning_rate=0.1):\n",
    "        layer_sizes = [self.input_size] + self.hidden_sizes + [self.output_size]\n",
    "\n",
    "        if not self.Ws:\n",
    "            for size1, size2 in zip(layer_sizes, layer_sizes[1:]):\n",
    "                W = np.random.randn(size1, size2)\n",
    "                b = np.random.randn(size2)\n",
    "                self.Ws.append(W)\n",
    "                self.bs.append(b)\n",
    "\n",
    "        # Training Loop\n",
    "        for epoch in range(epochs):\n",
    "            # Forward Pass\n",
    "            activations = [X]\n",
    "            zs = []\n",
    "            current_activation = X\n",
    "            for W, b in zip(self.Ws, self.bs):\n",
    "                z = current_activation @ W + b\n",
    "                zs.append(z)\n",
    "                current_activation = sigmoid(z)\n",
    "                activations.append(current_activation)\n",
    "            \n",
    "            output = activations[-1]\n",
    "\n",
    "            # Backpropagation\n",
    "            # Calculate output layer error\n",
    "            error = y - output\n",
    "            \n",
    "            # This is a simplified derivative for a squared error loss.\n",
    "            # It should be multiplied by the derivative of the activation function.\n",
    "            d_out = error * sigmoid_derivative(output) \n",
    "            \n",
    "            # Backpropagate the error\n",
    "            deltas = [d_out]\n",
    "            # Loop backward through the layers from the second-to-last layer\n",
    "            for i in range(len(self.Ws) - 1, 0, -1):\n",
    "                delta = deltas[-1] @ self.Ws[i].T * sigmoid_derivative(activations[i])\n",
    "                deltas.append(delta)\n",
    "            deltas.reverse()\n",
    "\n",
    "            # Update weights and biases\n",
    "            for i in range(len(self.Ws)):\n",
    "                # Reshape for matrix multiplication\n",
    "                # Check for batch vs single example and reshape accordingly\n",
    "                if len(X.shape) > 1: # Batch input\n",
    "                    # Update W using outer product of activations and deltas\n",
    "                    self.Ws[i] += learning_rate * activations[i].T @ deltas[i]\n",
    "                    self.bs[i] += learning_rate * np.sum(deltas[i], axis=0) # Sum deltas for batch\n",
    "                else: # Single input\n",
    "                    # Update W using outer product\n",
    "                    self.Ws[i] += learning_rate * np.outer(activations[i], deltas[i])\n",
    "                    self.bs[i] += learning_rate * deltas[i]\n",
    "\n",
    "    def predict(self, X):\n",
    "        h_in = X\n",
    "        for W, b in zip(self.Ws, self.bs):\n",
    "            z = h_in @ W + b\n",
    "            h_out = sigmoid(z) \n",
    "            h_in = h_out\n",
    "        return h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (20640,) (20640,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m nn = FNN(input_size=\u001b[32m8\u001b[39m, hidden_sizes=[\u001b[32m6\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m7\u001b[39m], output_size=\u001b[32m10\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m nn.predict(X_scaled)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mFNN.fit\u001b[39m\u001b[34m(self, X, y, epochs, learning_rate)\u001b[39m\n\u001b[32m     39\u001b[39m output = activations[-\u001b[32m1\u001b[39m]\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Calculate output layer error\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m error = \u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# This is a simplified derivative for a squared error loss.\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# It should be multiplied by the derivative of the activation function.\u001b[39;00m\n\u001b[32m     47\u001b[39m d_out = error * sigmoid_derivative(output) \n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (20640,) (20640,10) "
     ]
    }
   ],
   "source": [
    "nn = FNN(input_size=8, hidden_sizes=[6, 4, 2, 7], output_size=10)\n",
    "nn.fit(X_scaled, y)\n",
    "nn.predict(X_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
